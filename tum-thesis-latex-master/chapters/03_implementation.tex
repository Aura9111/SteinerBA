% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Implementation}\label{chapter:implementation}

\section{MST-Algorithm}

To implement the MST-Algorithm we first build the metric closure $\bar{G}$ of the input Graph $G$. Then we take the subgraph $\bar{G}_T$ of this metric closure containing all required terminal nodes $T$. In order to compute the metric closure of our input Graph we used the algorithm by Floyd Warshall \cite{hougardy2010floyd}, but any other all-pairs-shortest-path algorithm would suffice just fine.Using the subgraph as input for a minimum spanning tree algorithm, we receive a tree which now consists of only terminals and edges, that represent the shortest paths between them. In this implementation we are going to use Joseph Kruskal's minimum spanning tree algorithm \cite{kruskal1956shortest}, since it's simple, intuitive and is also easily reusable for building the minimum spanning forest we need to compute the loss of a Steiner tree. Kruskal's algorithm uses a sorted list of all available edges with ascending edge costs as well as a forest structure to construct the minimum spanning tree. It initially adds a tree for each terminal to the forest and then procedes to loop through the sorted edge list and checks for every edge, whether it could connect to different trees. If it does it is added to the forest, which reduces the number of trees in the forest by 1 and if it doesn't the algorithm just continues through the list. If the number of components in the forest reaches 1 the algorithm stops as a MST is already present. The only other way to terminate is if the entire list of edges has been exhausted and no MST has been found yet. In this case the construction of a MST would have been impossible since if an edge between two nodes doesn't exist in the metric closure of a Graph they would have to be in two separate components in the original Graph. In this case the desired MST can't be created. With this MST now created the final step is for us to replace the edges of the tree by the corresponding shortest paths in G and the outcome is our Steiner tree approximation.

\section{Berman-Ramaiyer-Algorithm}

The approximation algorithm by Berman and Ramaiyer is split into two phases which both maintain a spanning tree $M$ of $T$, which is initialized to the output of our MST-Algorithm. The first phase is called the evaluation phase and it uses the $prepareChange$ procedure, which we are going to look at shortly, to compute two sets of edges called the Add-Set $A$ and the Remove-Set $R$ for every $j$-element subset $\tau \subseteq T$, with $j$ being increased up to a maximum size bounded by the input number $k$. It then uses these sets to compute a $gain$ of $\tau$, by subtracting the cost of a SMT($\tau$) from the cost of the Remove-Set. If this $gain$ is greater than zero the Remove-Set is removed from M and every edge of the Add-Set has its cost reduced by $gain$ and is added to $M$ right afterwards. This marks a tentative preference to add SMT($\tau$). The subset $\tau$, the Remove-Set $R$ and the Add-Set $A$ are added to a stack $\sigma_j$ to be read in the construction phase.

\begin{algorithm}[ht]
$M = SMT(T)$\;						
\For{$j=3$ \KwTo $k$}{		
	$\sigma_j=\emptyset$\;						
	\ForEach{j-element subset $\tau\subseteq T$ }{	
		$[R, A]=prepareChange(M, \tau)$\;
		$gain= cost(R) - smt(\tau)$\;
 		\If{$gain>0$}{
   			Decrease the cost of each edge $\in A$ by $gain$\;
      			$M = M \backslash R \cup A$\;
      			$\sigma_j.push(\tau, R, A)$\;
  		}
 	}
}
\captionof{figure}{Evaluation Phase from Berman, Ramaiyer (Fig.2 \cite{BeRa94})}\label{fig:evaluationPseudo}
\end{algorithm}

The construction phase works on the output of the evaluation phase and starts by initializing the second spanning tree $N$, which will end up being the submitted solution, with the current version of $M$. It moves through the stacks $\sigma_j$ in opposite direction and pops entries from it until the stack is empty. Every entries data is used to revert the changes made to $M$ by subtracting the Add-Set and adding the Remove-Set. If all edges from the Add-Set are still present in $N$, they are removed and the SMT($\tau$) is added in their place. If there are only some but not all edges from $A$ remaining in $N$, each of these edges $e \subseteq (A\cap N)$ is replaced in $N$ with the minimal cost edge in $M$, that connects the two components created by removing $e$. After these changes have been applied for every entry of every stack the approximated minimal Steiner tree is present in $N$, while $M$ should have reverted to the original input of the evaluation phase, which was an MST-approximation of the minimal Steiner tree. 

\begin{algorithm}[ht]
$N=M$\;
\For{$j= k$ \KwTo $3$}{
	\While{$\sigma_j\neq\emptyset$}{
		$[\tau, R, A]=\sigma_j.pop()$\;
		$M=M\backslash A \cup R$\;
 		\eIf{$A \subseteq N$}{
   			$N=N\backslash A \cup SMT(\tau)$\;
  		}{
   			\ForEach{$e \in A \cap N$}{
				Find $f \in M$ of minimal cost such that $N \backslash e \cup f$ connects $T$\;
				$N= N \backslash e \cup f$\;
			}
  		}
 	}
}
\captionof{figure}{Construction Phase from Berman, Ramaiyer (Fig.3 \cite{BeRa94})}\label{fig:constructionPseudo}
\end{algorithm}

The procedure prepareChange is a recursive function, which assembles a Remove-Set $R$ and an Add-Set $A$ by adding the highest cost edge $e$, that connects two sets, which each contain at least one terminal node, to $R$. It creates a substitude edge $f$, which connects the components created by removing $e$ by connecting one terminal from each component. $f$ is added to the Add-Set $A$ and its cost will be changed later in the evaluation phase to mark a preference to connect these terminals using a Steiner tree of a subset including these terminals. Finally prepareChange will then be applied to the two components and the two results will be joined and returned. The recursion stops when the number of terminals in a component reaches one. In this case an the returned Remove-Set and Add-Set are both empty.

\begin{algorithm}[ht]
\SetKwFunction{prepCh}{prepareChange}
\prepCh{$M, \tau$}{
$R=\emptyset$\;
$A=\emptyset$\;
\eIf{$|\tau|==1$}{
	\KwRet [R, A]\;
}{
	Find an edge $e$ of maximum cost such that both the connected components of $M \backslash e$ contain a vertex of $\tau$\; 
	$[M_1, M_2]=M \backslash e$\;
	$[\tau_1, \tau_2]$ are the vertices of $\tau$ in $M_1, M_2$ respectively
	create an edge $f$ joining some $u \in \tau_1$ and some $v \in \tau_2$\;
	$cost(f)=cost(e)$\;
	$[R_1, A_1]=$\prepCh{$M_1, \tau_1$}\;
	$[R_2, A_2]=$\prepCh{$M_2, \tau_2$}\;
	$R=R_1 \cup R_2 \cup e$\;
	$A=A_1 \cup A_2 \cup f$\;
	\KwRet [R, A]\;
}
}
\captionof{figure}{prepareChange from Berman, Ramaiyer (Fig.1 \cite{BeRa94})}\label{fig:prepareChangePseudo}
\end{algorithm}

\section{Hougardy-Proemel-Algorithm}

Similar to the algorithm by Berman and Ramaiyer, the algorithm by Hougardy and Proemel \cite{HoPr99} starts of with an Steiner tree initialized with the MST-algorithm. It iteratively applies the $RGH(\alpha)$ algortihm by Karpinski and Zelikovsky \cite{karpinski1997new} with diminishing values for $\alpha$ in every iteration.$$\vec{\alpha}=(\alpha_1,\dots,\alpha_k) \text{ with } \alpha_1 \geq \dots \geq \alpha_k=0$$ 
The $RGH(\alpha)$ uses the greedy contraption framework \cite{karpinski1997new} with class $K$ including every $k$-restricted Steiner tree $B$ with $k\to\infty$ and the criterion function $f$: $$ f(B) = \frac{d(B) + \alpha * l(B)}{smt(T)-smt(T/B)} $$
The full $RGH(\alpha)$ the looks like described in \ref{fig:RGHPseudo}
By iteratively expanding the set of included nodes $IRGH$ manages to reduce the worst case performance and therefore the Performance Ratio and beat out all previously known approximation algorithms. It even includes Steiner nodes, that may increase the cost of the output tree, since their inclusion opens up more options within the next iteration. This leads to the approximation solution of this algortihm being either much better or slightly worse than its input, which is an MST-approximation.  While the results of the Berman-Ramaiyer-algorithm remained very close to the input MST-approximation with little improvements built into it, the $IRGH$ have vastly more Steiner nodes to the point. Judging by purely its superior performance ratio (which can be referenced in table \ref{tab:pr}) $IRGH$ looks like the better algorithm choice. 

\begin{algorithm}[hb]
$S=T$\;
\While{$smt(T)>0$} {
	\ForEach{k-restricted Steiner tree $B$}{
		\If{$B$ minimizes $f$}{
			$S=S\cup \text{Steiner points in } B$\;
			$T=T/B$\;
		}
	}
}
\KwRet $S$\;

\captionof{figure}{$RGH(\alpha)$ by Karpinski and Zelikovsky \cite{karpinski1997new}}\label{fig:RGHPseudo}
\end{algorithm}


\begin{algorithm}[hb]
$T_0=T=$ terminals of $G$\;
\For{$i=1$ \KwTo $k$} {
	apply $RGH(\alpha_i)$ to $T_i-1$ to get $S$\;
	$T_i=T_i-1 \cup \{$Steiner points of $S\}$\;
}
\KwRet $SMT(T_k)$\;
\captionof{figure}{$IRGH(\vec{\alpha}$ by Hougardy, Proemel \cite{HoPr99}}\label{fig:IRGHPseudo}
\end{algorithm}
