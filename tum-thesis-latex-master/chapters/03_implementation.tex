% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Implementation}\label{chapter:implementation}

The source code for this thesis can be referenced here \cite{githublink}

\section{MST-Algorithm}

To implement the MST-algorithm we first build the metric closure $\bar{G}$ of the input graph $G$. Then we take the subgraph $\bar{G}_T$ of this metric closure containing all required terminal nodes $T$. In order to compute the metric closure of our input graph we used the algorithm by Floyd Warshall \cite{hougardy2010floyd}, but any other all-pairs-shortest-path algorithm would suffice just fine. Using the subgraph as input for a minimum spanning tree algorithm, we receive a tree which now consists of only terminals and edges that represent the shortest paths between them. In this implementation we are going to use Joseph Kruskal's minimum spanning tree algorithm \cite{kruskal1956shortest}, since it is simple, intuitive and is also easily reusable for building the minimum spanning forest we need to compute the loss of a Steiner tree. Kruskal's algorithm uses a sorted list of all available edges with ascending edge costs as well as a forest structure to construct the minimum spanning tree. It initially adds a tree for each terminal to the forest and then procedes to loop through the sorted edge list and checks for every edge, whether it could connect to different trees. If it does, it is added to the forest, which reduces the number of trees in the forest by one and if it does not, the algorithm just continues through the list. If the number of components in the forest reaches one the algorithm stops, since a MST is already present. The only other way to terminate is, if the entire list of edges has been exhausted and no MST has been found yet. In this case the construction of a MST would have been impossible, since if an edge between two nodes does not exist in the metric closure of a graph they would have to be in two separate components in the original graph. In this case the desired MST cannot be created. With this MST now created the final step is for us to replace the edges of the tree by the corresponding shortest paths in G and the outcome is our Steiner tree approximation.

\section{Berman-Ramaiyer-Algorithm}

The approximation algorithm by Berman and Ramaiyer is split into two phases which both maintain a spanning tree $M$ of $T$, which is initialized to the output of our MST-algorithm. The first phase is called the evaluation phase and it uses the $prepareChange$ procedure, which we are going to look at shortly, to compute two sets of edges called the add-set $A$ and the remove-set $R$ for every $j$-element subset $\tau \subseteq T$, with $j$ being increased up to a maximum size bounded by the input number $k$. It then uses these sets to compute a $gain$ of $\tau$, by subtracting the cost of a SMT($\tau$) from the cost of the remove-set. If this $gain$ is greater than zero the remove-set is removed from M and every edge of the add-set has its cost reduced by $gain$ and is added to $M$ right afterwards. This marks a tentative preference to add SMT($\tau$). The subset $\tau$, the Remove-Set $R$ and the Add-Set $A$ are added to a stack $\sigma_j$ to be read in the construction phase.

\begin{algorithm}[h!]
$M = SMT(T)$\;						
\For{$j=3$ \KwTo $k$}{		
	$\sigma_j=\emptyset$\;						
	\ForEach{j-element subset $\tau\subseteq T$ }{	
		$[R, A]=prepareChange(M, \tau)$\;
		$gain= cost(R) - smt(\tau)$\;
 		\If{$gain>0$}{
   			Decrease the cost of each edge $\in A$ by $gain$\;
      			$M = M \backslash R \cup A$\;
      			$\sigma_j.push(\tau, R, A)$\;
  		}
 	}
}
\captionof{figure}{Evaluation phase from Berman, Ramaiyer (Fig.2 \cite{BeRa94})}\label{fig:evaluationPseudo}
\end{algorithm}

The construction phase works on the output of the evaluation phase and starts by initializing the second spanning tree $N$, which will end up being the submitted solution, with the current version of $M$. It moves through the stacks $\sigma_j$ in opposite direction and pops entries from it until the stack is empty. Every entry's data is used to revert the changes made to $M$ by subtracting the add-set and adding the remove-set. If all edges from the add-set are still present in $N$, they are removed and the SMT($\tau$) is added in their place. If there are only some, but not all edges from $A$ remaining in $N$, each of these edges $e \subseteq (A\cap N)$ is replaced in $N$ with the minimal cost edge in $M$, that connects the two components created by removing $e$. After these changes have been applied for every entry of every stack the approximated minimal Steiner tree is present in $N$, while $M$ should have reverted to the original input of the evaluation phase, which was an MST-approximation of the minimal Steiner tree. 

\begin{algorithm}[h!]
$N=M$\;
\For{$j= k$ \KwTo $3$}{
	\While{$\sigma_j\neq\emptyset$}{
		$[\tau, R, A]=\sigma_j.pop()$\;
		$M=M\backslash A \cup R$\;
 		\eIf{$A \subseteq N$}{
   			$N=N\backslash A \cup SMT(\tau)$\;
  		}{
   			\ForEach{$e \in A \cap N$}{
				Find $f \in M$ of minimal cost such that $N \backslash e \cup f$ connects $T$\;
				$N= N \backslash e \cup f$\;
			}
  		}
 	}
}
\captionof{figure}{Construction Phase from Berman, Ramaiyer (Fig.3 \cite{BeRa94})}\label{fig:constructionPseudo}
\end{algorithm}

The procedure $prepareChange$ is a recursive function, which assembles a remove-set $R$ and an add-set $A$ by adding the highest cost edge $e$ that connects two sets each containing at least one terminal node, to $R$. Removing $e$ creates two components, which are reconnected by creating a substitude edge $f$ that connects one terminal from each component. $f$ is added to the add-set $A$ and its cost will be changed later in the evaluation phase to mark a preference to connect these terminals using a Steiner tree of a subset including these terminals. Finally $prepareChange$ will then be applied to the two components to obtain two results that will be joined and returned. The recursion stops when the number of terminals in a component reaches one. In this case the returned remove-set and add-set are both empty.


\begin{algorithm}[ht!]
\SetKwFunction{prepCh}{$prepareChange$}
\prepCh{$M, \tau$}{
$R=\emptyset$\;
$A=\emptyset$\;
\eIf{$|\tau|==1$}{
	\KwRet [R, A]\;
}{
	Find an edge $e$ of maximum cost such that both the connected components of $M \backslash e$ contain a vertex of $\tau$\; 
	$[M_1, M_2]=M \backslash e$\;
	$[\tau_1, \tau_2]$ are the vertices of $\tau$ in $M_1, M_2$ respectively
	create an edge $f$ joining some $u \in \tau_1$ and some $v \in \tau_2$\;
	$cost(f)=cost(e)$\;
	$[R_1, A_1]=$\prepCh{$M_1, \tau_1$}\;
	$[R_2, A_2]=$\prepCh{$M_2, \tau_2$}\;
	$R=R_1 \cup R_2 \cup e$\;
	$A=A_1 \cup A_2 \cup f$\;
	\KwRet [R, A]\;
}
}
\captionof{figure}{$prepareChange$ from Berman, Ramaiyer (Fig.1 \cite{BeRa94})}\label{fig:prepareChangePseudo}
\end{algorithm}

\section{Hougardy-Proemel-Algorithm}

Similar to the algorithm by Berman and Ramaiyer, the algorithm by Hougardy and Proemel \cite{HoPr99} starts of with a Steiner tree initialized with the MST-algorithm. It iteratively applies the $RGH(\alpha)$ algortihm by Karpinski and Zelikovsky \cite{karpinski1997new} with diminishing values for $\alpha$ in every iteration.$$\vec{\alpha}=(\alpha_1,\dots,\alpha_k) \text{ with } \alpha_1 \geq \dots \geq \alpha_k=0$$ 
The $RGH(\alpha)$ uses the greedy contraption framework \cite{karpinski1997new} with class $K$ including every $k$-restricted Steiner tree $B$ with $k\to\infty$ and the criterion function $f$: $$ f(B) = \frac{d(B) + \alpha * l(B)}{smt(T)-smt(T/B)} $$
The full $RGH(\alpha)$ looks like described in figure \ref{fig:RGHPseudo}.
By iteratively expanding the set of included nodes $IRGH$ manages to reduce the worst case performance and therefore the performance ratio and beat out all previously known approximation algorithms. It even includes Steiner nodes, that may increase the cost of the output tree, since their inclusion opens up more options within the next iteration. This leads to the approximation solution of this algortihm being either much better or slightly worse than its input, which is an MST-approximation.  While the results of the Berman-Ramaiyer-algorithm remained very close to the input MST-approximation, with little improvements built into it, the $IRGH$ have vastly more Steiner nodes by comparison. Judging by purely its superior performance ratio (which can be referenced in table \ref{tab:pr}) $IRGH$ looks like the better algorithm choice. An additional thing to note for the $IRGH$-algorithm is that the runtime of it is very long compared to the other two algorithms. Since the set $\vec{\alpha}$ could have potentially infinite values $\alpha_i$ the runtime can be infinite as well. For this thesis we ran with the optimal $\alpha$-values for $k=3$ most of the time. The results for $k=6$ proved to be the same for all of the relevant testcases and the computation time was greatly increased. It is important to note that the values, that appear later in this thesis could possibly be improved by simply picking a higher value for $k$ and therefore having more iterations of $RGH$, but especially, since $IRGH$ already has the best performance ratio and the longest runtime it should not be necessary to increase $k$ in order to achieve representative results for this algorihtm.  

\begin{algorithm}[h!]
$S=T$\;
\While{$smt(T)>0$} {
	\ForEach{k-restricted Steiner tree $B$}{
		\If{$B$ minimizes $f$}{
			$S=S\cup \text{Steiner points in } B$\;
			$T=T/B$\;
		}
	}
}
\KwRet $S$\;

\captionof{figure}{$RGH(\alpha)$ by Karpinski and Zelikovsky \cite{karpinski1997new}}\label{fig:RGHPseudo}
\end{algorithm}


\begin{algorithm}[h!]
$T_0=T=$ terminals of $G$\;
\For{$i=1$ \KwTo $k$} {
	apply $RGH(\alpha_i)$ to $T_i-1$ to get $S$\;
	$T_i=T_i-1 \cup \{$Steiner points of $S\}$\;
}
\KwRet $SMT(T_k)$\;
\captionof{figure}{$IRGH(\vec{\alpha}$) by Hougardy, Proemel \cite{HoPr99}}\label{fig:IRGHPseudo}
\end{algorithm}
